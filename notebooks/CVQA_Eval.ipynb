{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70d9dc38-e5e1-4f86-8489-26924f45da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e3e88ae-df51-4131-9c52-ddae5f3cb4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAVA_DIRECTORY_PATH = '/home/user/Documents/GitHub/LLaVA'\n",
    "\n",
    "sys.path.insert(0,LLAVA_DIRECTORY_PATH)\n",
    "sys.path.insert(0, Path(LLAVA_DIRECTORY_PATH).joinpath('playground/finetuning').absolute().as_posix())  # For importing the finetuning specific files\n",
    "\n",
    "from llava.eval.maya.eval_utils import load_maya_model, get_single_sample_prediction, generate_cvqa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548a4f80-5372-46dc-8748-e0dcca3ae4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(cvqa_folder / filelist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c54fa55-d773-454a-a8b5-1fd39c90444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ID</th>\n",
       "      <th>Subset</th>\n",
       "      <th>Question</th>\n",
       "      <th>Translated Question</th>\n",
       "      <th>Options</th>\n",
       "      <th>Translated Options</th>\n",
       "      <th>Label</th>\n",
       "      <th>Category</th>\n",
       "      <th>Image Type</th>\n",
       "      <th>Image Source</th>\n",
       "      <th>License</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>5865910664272406812_1</td>\n",
       "      <td>('Chinese', 'China')</td>\n",
       "      <td>图中的‘318’代表着什么意思？</td>\n",
       "      <td>What does ‘318’ in the picture mean?</td>\n",
       "      <td>[海拔, 日期, 路程长度, 国道]</td>\n",
       "      <td>[Altitude, Date, Length of the road, National ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Vehicles and Transportation</td>\n",
       "      <td>Self</td>\n",
       "      <td>Self-research_only</td>\n",
       "      <td>CC BY-NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>5865925454277921835_2</td>\n",
       "      <td>('Filipino', 'Philippines')</td>\n",
       "      <td>Ano ang kilalang tawag sa babae sa larawan?</td>\n",
       "      <td>The woman in the photo is most commonly known as?</td>\n",
       "      <td>[Ina ng Pilipinas, Ina ng Manila, Ina ng Rebol...</td>\n",
       "      <td>[Mother of the Philippines, Mother of Manila, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Traditions / art / history</td>\n",
       "      <td>External</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Melcho...</td>\n",
       "      <td>Public domain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>5865743574274453408_0</td>\n",
       "      <td>('Korean', 'South Korea')</td>\n",
       "      <td>이 사진에서 사람들이 참여하고 있는 활동은 무엇입니까?</td>\n",
       "      <td>What activity are the people participating in ...</td>\n",
       "      <td>[마라톤 , 조정 , 댄스 스포츠, 전통 무술]</td>\n",
       "      <td>[Marathon, Rowing, Dance sports, Traditional m...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Sports and recreation</td>\n",
       "      <td>External</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:KOCIS_...</td>\n",
       "      <td>CC BY-SA 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>5865940594277100134_1</td>\n",
       "      <td>('Spanish', 'Spain')</td>\n",
       "      <td>¿Quién pintó este cuadro?</td>\n",
       "      <td>Who painted this picture?</td>\n",
       "      <td>[Pablo Picasso, Franciso de Goya, Bartolomé Es...</td>\n",
       "      <td>[Pablo Picasso, Franciso de Goya, Bartolome Es...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Traditions / art / history</td>\n",
       "      <td>External</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/File:Maja_d...</td>\n",
       "      <td>Public domain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n",
       "      <td>5865910684273901143_0</td>\n",
       "      <td>('Chinese', 'China')</td>\n",
       "      <td>这是什么植物？</td>\n",
       "      <td>What plant is this?</td>\n",
       "      <td>[郁金香, 樱花, 牡丹花, 玫瑰花]</td>\n",
       "      <td>[Tulip, Cherry blossoms, Peony, Roses]</td>\n",
       "      <td>-1</td>\n",
       "      <td>Plants and animal</td>\n",
       "      <td>External</td>\n",
       "      <td>https://www.flickr.com/photos/a3u9c2/187996110...</td>\n",
       "      <td>Attribution-NoDerivs License</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image                     ID  \\\n",
       "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  5865910664272406812_1   \n",
       "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  5865925454277921835_2   \n",
       "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  5865743574274453408_0   \n",
       "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  5865940594277100134_1   \n",
       "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  5865910684273901143_0   \n",
       "\n",
       "                        Subset                                     Question  \\\n",
       "0         ('Chinese', 'China')                             图中的‘318’代表着什么意思？   \n",
       "1  ('Filipino', 'Philippines')  Ano ang kilalang tawag sa babae sa larawan?   \n",
       "2    ('Korean', 'South Korea')               이 사진에서 사람들이 참여하고 있는 활동은 무엇입니까?   \n",
       "3         ('Spanish', 'Spain')                    ¿Quién pintó este cuadro?   \n",
       "4         ('Chinese', 'China')                                      这是什么植物？   \n",
       "\n",
       "                                 Translated Question  \\\n",
       "0               What does ‘318’ in the picture mean?   \n",
       "1  The woman in the photo is most commonly known as?   \n",
       "2  What activity are the people participating in ...   \n",
       "3                          Who painted this picture?   \n",
       "4                                What plant is this?   \n",
       "\n",
       "                                             Options  \\\n",
       "0                                 [海拔, 日期, 路程长度, 国道]   \n",
       "1  [Ina ng Pilipinas, Ina ng Manila, Ina ng Rebol...   \n",
       "2                         [마라톤 , 조정 , 댄스 스포츠, 전통 무술]   \n",
       "3  [Pablo Picasso, Franciso de Goya, Bartolomé Es...   \n",
       "4                                [郁金香, 樱花, 牡丹花, 玫瑰花]   \n",
       "\n",
       "                                  Translated Options  Label  \\\n",
       "0  [Altitude, Date, Length of the road, National ...     -1   \n",
       "1  [Mother of the Philippines, Mother of Manila, ...     -1   \n",
       "2  [Marathon, Rowing, Dance sports, Traditional m...     -1   \n",
       "3  [Pablo Picasso, Franciso de Goya, Bartolome Es...     -1   \n",
       "4             [Tulip, Cherry blossoms, Peony, Roses]     -1   \n",
       "\n",
       "                      Category Image Type  \\\n",
       "0  Vehicles and Transportation       Self   \n",
       "1   Traditions / art / history   External   \n",
       "2        Sports and recreation   External   \n",
       "3   Traditions / art / history   External   \n",
       "4            Plants and animal   External   \n",
       "\n",
       "                                        Image Source  \\\n",
       "0                                 Self-research_only   \n",
       "1  https://commons.wikimedia.org/wiki/File:Melcho...   \n",
       "2  https://commons.wikimedia.org/wiki/File:KOCIS_...   \n",
       "3  https://commons.wikimedia.org/wiki/File:Maja_d...   \n",
       "4  https://www.flickr.com/photos/a3u9c2/187996110...   \n",
       "\n",
       "                        License  \n",
       "0                      CC BY-NC  \n",
       "1                 Public domain  \n",
       "2                  CC BY-SA 2.0  \n",
       "3                 Public domain  \n",
       "4  Attribution-NoDerivs License  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b4cb0-8b6c-4005-b335-7c9e516bf13b",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e65e08db-8b89-4cfe-8b29-455fd357da31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1e280bda654ba4b76a3b897aeae294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/user/Documents/GitHub/maya_full_ft were not used when initializing LlavaCohereForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_bias', 'model.vision_tower.vision_tower.vision_model.head.attention.in_proj_weight', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.head.attention.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.head.layernorm.bias', 'model.vision_tower.vision_tower.vision_model.head.layernorm.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.head.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.head.probe', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight']\n",
      "- This IS expected if you are initializing LlavaCohereForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaCohereForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_base = 'CohereForAI/aya-23-8B'\n",
    "model_path = '/home/user/Documents/GitHub/maya_full_ft'\n",
    "mode = 'finetuned'\n",
    "\n",
    "maya_model = load_maya_model(model_base, model_path, mode = mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedae3f2-d724-42f1-b110-68c51ffbaa65",
   "metadata": {},
   "source": [
    "### Testing on Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8c6803b3-413b-4a87-ac1e-cd84857f625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "sample = df.iloc[index]\n",
    "\n",
    "image = sample.image['bytes']\n",
    "\n",
    "user_question = generate_cvqa_prompt(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9d1ef20-8e63-4bed-a239-70a31dd9a2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3\n"
     ]
    }
   ],
   "source": [
    "output = get_single_sample_prediction(maya_model, image, user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5b140-9af7-4cfe-9aac-cadcddc9258c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Running CVQA eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4101d736-044c-4608-93d2-b64b879f1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvqa_folder = Path('/home/user/Documents/Datasets/CVQA/')\n",
    "\n",
    "filelist = os.listdir(cvqa_folder)\n",
    "if  '.ipynb_checkpoints' in filelist:\n",
    "    filelist.remove('.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5329234b-14ff-4a25-b895-02683c0756b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1037/1037 [03:02<00:00,  5.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1038/1038 [03:12<00:00,  5.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1037/1037 [03:07<00:00,  5.54it/s]\n",
      " 17%|████████████████████                                                                                                    | 173/1037 [00:30<02:30,  5.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1037/1037 [03:09<00:00,  5.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1037/1037 [03:00<00:00,  5.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1038/1038 [03:01<00:00,  5.71it/s]\n",
      "  5%|██████                                                                                                                   | 52/1038 [00:10<03:14,  5.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1038/1038 [03:05<00:00,  5.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1037/1037 [03:07<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "skipped_answers = []\n",
    "\n",
    "for file in filelist:\n",
    "\n",
    "    df = pd.read_parquet(cvqa_folder / file)\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "\n",
    "        sample = df.iloc[i]\n",
    "        \n",
    "        sample_id = sample.ID\n",
    "        image = sample.image['bytes']\n",
    "        user_question = generate_cvqa_prompt(sample)\n",
    "    \n",
    "        output = get_single_sample_prediction(maya_model, image, user_question)\n",
    "    \n",
    "        try:\n",
    "            output = int(output)\n",
    "        except:\n",
    "            skipped_answers.append([sample_id, output, user_question])\n",
    "            output = 0\n",
    "            \n",
    "        results.append([sample_id, output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5625f5d2-5c4f-4033-bc72-87ddab7f2fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10374"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a90b4671-3ddb-4942-af87-71651a4ccc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5865910664272406812_1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5865925454277921835_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5865743574274453408_0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5865940594277100134_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5865910684273901143_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID  Prediction\n",
       "0  5865910664272406812_1           3\n",
       "1  5865925454277921835_2           0\n",
       "2  5865743574274453408_0           3\n",
       "3  5865940594277100134_1           1\n",
       "4  5865910684273901143_0           0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsDf = pd.DataFrame(results)\n",
    "resultsDf.columns = ['ID','Prediction'] \n",
    "resultsDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d80cb9bb-20a4-4e73-9d24-2ae8ab758eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf.to_csv('cvqa_predictions.csv',index = False, index_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "232f9415-5e8f-49e5-911f-ca3e182738c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user/Documents/GitHub/LLaVA/playground/eval'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed9855-16ec-496e-9711-34172fe75286",
   "metadata": {},
   "source": [
    "#### To cover missed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059ab8b-f8dc-497b-a586-6617c384fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_result_ids = [x[0] for x in results]\n",
    "\n",
    "# # results_add = []\n",
    "\n",
    "# for file in filelist:\n",
    "\n",
    "#     df = pd.read_parquet(cvqa_folder / file)\n",
    "\n",
    "#     for i in tqdm(range(len(df))):\n",
    "\n",
    "#         sample = df.iloc[i]\n",
    "        \n",
    "#         sample_id = sample.ID\n",
    "\n",
    "#         if sample_id in existing_result_ids:\n",
    "#             continue\n",
    "        \n",
    "#         image = sample.image['bytes']\n",
    "#         user_question = generate_cvqa_prompt(sample)\n",
    "    \n",
    "#         output = get_single_sample_prediction(maya_model, image, user_question)\n",
    "    \n",
    "#         try:\n",
    "#             output = int(output)\n",
    "#         except:\n",
    "#             skipped_answers.append([sample_id, output, user_question])\n",
    "#             output = 0\n",
    "            \n",
    "#         results.append([sample_id, output])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
